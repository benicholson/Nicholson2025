{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyNq/Z9FT6CA+b4+f/Hwfcql"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JecWJLjknf8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1744137927515,
     "user_tz": 300,
     "elapsed": 3620,
     "user": {
      "displayName": "Ben Nicholson",
      "userId": "16080346744350514230"
     }
    },
    "outputId": "9af23c75-9c72-49f3-a084-48115289cea3"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# final model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(\"reddit_sample_for_sentiment_with_labels_v7.csv\")\n",
    "df = df.dropna(subset=[\"Text_Preprocessed\", \"Stance\"])\n",
    "\n",
    "label_map = { -1: 0, 0: 1, 1: 2 }\n",
    "df[\"sentiment_label\"] = df[\"Stance\"].map(label_map)\n",
    "\n",
    "# mark quotes\n",
    "def mark_quoted_lines(text):\n",
    "    processed_lines = []\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\">\"):\n",
    "            quote_content = line.strip()[1:].strip()\n",
    "            processed_lines.append(f\"[QUOTE] {quote_content} [/QUOTE]\")\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "    return \"\\n\".join(processed_lines)\n",
    "\n",
    "df[\"Text_quoted_marked\"] = df[\"Text_Preprocessed\"].apply(mark_quoted_lines)\n",
    "\n",
    "# splitting data\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"sentiment_label\"])\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42, stratify=train_val_df[\"sentiment_label\"])\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# dataset class\n",
    "class SimpleSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.encodings = tokenizer(df[\"Text_quoted_marked\"].tolist(), truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "        self.labels = torch.tensor(df[\"sentiment_label\"].tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# create datasets\n",
    "train_dataset = SimpleSentimentDataset(train_df, tokenizer)\n",
    "val_dataset = SimpleSentimentDataset(val_df, tokenizer)\n",
    "test_dataset = SimpleSentimentDataset(test_df, tokenizer)\n",
    "\n",
    "# class weights\n",
    "class_weights_tensor = torch.tensor([2.0, 1.0, 2.0], dtype=torch.float)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=3, ignore_mismatched_sizes=True)\n",
    "model.classifier.weight.data = model.classifier.weight.data.clone()\n",
    "\n",
    "# custom trainer\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # âœ… Move weights to the model's current device\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.tensor(logits).argmax(dim=-1).numpy()\n",
    "    labels = torch.tensor(labels).numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None, zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1.mean(),\n",
    "        \"precision_0\": precision[0], \"recall_0\": recall[0], \"f1_0\": f1[0],\n",
    "        \"precision_1\": precision[1], \"recall_1\": recall[1], \"f1_1\": f1[1],\n",
    "        \"precision_2\": precision[2], \"recall_2\": recall[2], \"f1_2\": f1[2],\n",
    "    }\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results_sentiment_only\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = WeightedLossTrainer(\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# fine tune\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./results_sentiment_finetune\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "finetune_trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "finetune_trainer.train()\n",
    "\n",
    "# evaluate\n",
    "logits, labels = finetune_trainer.predict(test_dataset)[:2]\n",
    "probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "labels = torch.tensor(labels).numpy()\n",
    "preds = np.argmax(probs, axis=1)\n",
    "\n",
    "# save probability\n",
    "pred_df = pd.DataFrame({\n",
    "    \"text\": test_df[\"Text_Preprocessed\"],\n",
    "    \"true_label\": labels,\n",
    "    \"pred_label\": preds,\n",
    "    \"prob_0\": probs[:, 0],\n",
    "    \"prob_1\": probs[:, 1],\n",
    "    \"prob_2\": probs[:, 2],\n",
    "})\n",
    "pred_df.to_csv(\"sentiment_predictions.csv\", index=False)\n",
    "\n",
    "# report outputs\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "\n",
    "# save model\n",
    "save_path = \"sentiment_model_final\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model and tokenizer saved to {save_path}\")"
   ],
   "metadata": {
    "id": "Nrxo-ppnPfxB",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1744138682778,
     "user_tz": 300,
     "elapsed": 597242,
     "user": {
      "displayName": "Ben Nicholson",
      "userId": "16080346744350514230"
     }
    },
    "outputId": "00c7502f-ed39-4cde-d4ec-bad9f0ce7cbb"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-7-09045483aeca>:117: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: No netrc file found, creating one.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbenicholson\u001B[0m (\u001B[33mbenicholson-university-of-chicago\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/drive/.shortcut-targets-by-id/1peWGN9jx9XQel3yAY-avJUflFaFUlH_X/Thesis/Comments coding/wandb/run-20250408_184830-rgev3jvl</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benicholson-university-of-chicago/huggingface/runs/rgev3jvl' target=\"_blank\">./results_sentiment_only</a></strong> to <a href='https://wandb.ai/benicholson-university-of-chicago/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/benicholson-university-of-chicago/huggingface' target=\"_blank\">https://wandb.ai/benicholson-university-of-chicago/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/benicholson-university-of-chicago/huggingface/runs/rgev3jvl' target=\"_blank\">https://wandb.ai/benicholson-university-of-chicago/huggingface/runs/rgev3jvl</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2025' max='2025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2025/2025 06:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>Precision 1</th>\n",
       "      <th>Recall 1</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>Precision 2</th>\n",
       "      <th>Recall 2</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.083300</td>\n",
       "      <td>1.120531</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.145552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.262570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.415929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.098000</td>\n",
       "      <td>1.060708</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.376370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668342</td>\n",
       "      <td>0.696335</td>\n",
       "      <td>0.682051</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>0.447059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.822200</td>\n",
       "      <td>0.965715</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.540597</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.471264</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.675393</td>\n",
       "      <td>0.701087</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.449438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.552300</td>\n",
       "      <td>1.002746</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.572170</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.514970</td>\n",
       "      <td>0.723757</td>\n",
       "      <td>0.685864</td>\n",
       "      <td>0.704301</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.478723</td>\n",
       "      <td>0.497238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.405400</td>\n",
       "      <td>1.123241</td>\n",
       "      <td>0.619444</td>\n",
       "      <td>0.590246</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.670157</td>\n",
       "      <td>0.697548</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.627660</td>\n",
       "      <td>0.543779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-7-09045483aeca>:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  finetune_trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 02:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>F1 0</th>\n",
       "      <th>Precision 1</th>\n",
       "      <th>Recall 1</th>\n",
       "      <th>F1 1</th>\n",
       "      <th>Precision 2</th>\n",
       "      <th>Recall 2</th>\n",
       "      <th>F1 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>1.320264</td>\n",
       "      <td>0.630556</td>\n",
       "      <td>0.609457</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.540146</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.612565</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.483444</td>\n",
       "      <td>0.776596</td>\n",
       "      <td>0.595918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.225900</td>\n",
       "      <td>1.403127</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.609130</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.541353</td>\n",
       "      <td>0.733696</td>\n",
       "      <td>0.706806</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.566038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.52       186\n",
      "           1       0.77      0.60      0.67       478\n",
      "           2       0.47      0.69      0.56       235\n",
      "\n",
      "    accuracy                           0.60       899\n",
      "   macro avg       0.59      0.60      0.58       899\n",
      "weighted avg       0.64      0.60      0.61       899\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 95  36  55]\n",
      " [ 62 285 131]\n",
      " [ 25  47 163]]\n",
      "Model and tokenizer saved to sentiment_model_final\n"
     ]
    }
   ]
  }
 ]
}
