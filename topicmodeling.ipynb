{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(\"immigration_comments_with_period_label_updated.csv\")\n",
    "\n",
    "# Expecting columns: ['City', 'Period_Label', 'Text']\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SentenceTransformer(\"Linq-AI-Research/Linq-Embed-Mistral\", device=device)\n",
    "\n",
    "\n",
    "### generate embeddings\n",
    "\n",
    "def generate_ling_mistral_embeddings(texts, chunk_size=30000, batch_size=32):\n",
    "    final_embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        chunks = textwrap.wrap(text, width=chunk_size)\n",
    "        chunk_embeddings = []\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i+batch_size]\n",
    "            batch_emb = model.encode(\n",
    "                batch, convert_to_numpy=True, normalize_embeddings=True, device=device\n",
    "            )\n",
    "            chunk_embeddings.extend(batch_emb)\n",
    "        avg_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "        final_embeddings.append(avg_embedding)\n",
    "    return np.array(final_embeddings)\n",
    "\n",
    "# embeddings generated\n",
    "embeddings = generate_ling_mistral_embeddings(df['Text'].tolist())\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df.to_csv(\"ling_mistral_embeddings.csv\", index=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# load embeddings\n",
    "df = pd.read_csv(\"immigration_comments_with_period_label_updated.csv\")\n",
    "\n",
    "# exclude outliers, don't exclude deleted\n",
    "df = df[(df['Outlier'] == False) & (df['Username'] != '[deleted]')].reset_index(drop=True)\n",
    "\n",
    "# load embeddings\n",
    "embeddings = pd.read_csv(\"ling_mistral_embeddings.csv\").values\n",
    "embeddings = embeddings[df.index]\n",
    "\n",
    "# UMAP dimensionality reduction\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.0, metric='cosine', random_state=42)\n",
    "embedding_umap = reducer.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# HBDScan clustering\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embedding_umap)\n",
    "df['Topic_Label'] = labels\n",
    "\n",
    "# represent topics\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df['Text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "topics_summary = []\n",
    "\n",
    "for topic in set(labels):\n",
    "    if topic == -1:\n",
    "        continue\n",
    "    topic_indices = np.where(labels == topic)[0]\n",
    "    topic_comments = df.iloc[topic_indices]['Text'].tolist()\n",
    "\n",
    "    topic_tfidf = X[topic_indices].mean(axis=0)\n",
    "    top_word_indices = np.array(topic_tfidf).flatten().argsort()[-10:][::-1]\n",
    "    common_words = [feature_names[i] for i in top_word_indices]\n",
    "\n",
    "    topic_embeddings = embedding_umap[topic_indices]\n",
    "    centroid = np.mean(topic_embeddings, axis=0)\n",
    "    distances = np.linalg.norm(topic_embeddings - centroid, axis=1)\n",
    "    rep_comment_indices = distances.argsort()[:5]\n",
    "    representative_comments = [topic_comments[i] for i in rep_comment_indices]\n",
    "\n",
    "    topics_summary.append({\n",
    "        \"Topic_Label\": topic,\n",
    "        \"Common_Words\": \", \".join(common_words),\n",
    "        \"Representative_Comments\": \" | \".join(representative_comments)\n",
    "    })\n",
    "\n",
    "# add composition\n",
    "df['City_Period'] = df['City'] + '_' + df['Period_Label']\n",
    "all_city_periods = df['City_Period'].unique()\n",
    "\n",
    "for topic in topics_summary:\n",
    "    topic_label = topic['Topic_Label']\n",
    "    topic_df = df[df['Topic_Label'] == topic_label]\n",
    "\n",
    "\n",
    "\n",
    "    # composition based on stance\n",
    "    stance_counts = topic_df['Stance'].value_counts(normalize=True).to_dict()\n",
    "    topic['Prop_Positive'] = stance_counts.get(1, 0.0)\n",
    "    topic['Prop_Neutral'] = stance_counts.get(0, 0.0)\n",
    "    topic['Prop_Negative'] = stance_counts.get(-1, 0.0)\n",
    "\n",
    "    # city period composition\n",
    "    counts = topic_df['City_Period'].value_counts(normalize=True).to_dict()\n",
    "    for cp in all_city_periods:\n",
    "        topic[f\"Prop_{cp}\"] = counts.get(cp, 0.0)\n",
    "\n",
    "# save results\n",
    "df.to_csv(\"immigration_comments_with_topics.csv\", index=False)\n",
    "pd.DataFrame(topics_summary).to_csv(\"topic_summaries_enriched.csv\", index=False)"
   ],
   "id": "57da2d7c422b14ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T16:50:50.793620Z",
     "start_time": "2025-05-02T16:50:50.173562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# load and filter\n",
    "df = pd.read_csv(\"immigration_comments_with_topics.csv\")\n",
    "df = df[(df[\"2025\"] == False) & (df[\"Outlier\"] == False)].copy()\n",
    "\n",
    "# clean\n",
    "df[\"Text\"] = df[\"Text\"].fillna(\"\").str.lower().str.strip()\n",
    "\n",
    "# topics\n",
    "topics = [20, 28, 122, 124, 228]\n",
    "cities = [\"Chicago\", \"Denver\", \"New York City\"]\n",
    "\n",
    "# function\n",
    "def compare_topic_shift_by_city(city_df):\n",
    "    results = []\n",
    "    for topic in topics:\n",
    "        c = len(city_df[(city_df[\"Period_Label\"] == \"Control\") & (city_df[\"Topic_Label\"] == topic)])\n",
    "        e = len(city_df[(city_df[\"Period_Label\"] == \"Experimental\") & (city_df[\"Topic_Label\"] == topic)])\n",
    "        c_total = len(city_df[city_df[\"Period_Label\"] == \"Control\"])\n",
    "        e_total = len(city_df[city_df[\"Period_Label\"] == \"Experimental\"])\n",
    "\n",
    "        control_pct = (c / c_total) * 100 if c_total else 0\n",
    "        experimental_pct = (e / e_total) * 100 if e_total else 0\n",
    "        raw_change = experimental_pct - control_pct\n",
    "\n",
    "        if control_pct == 0:\n",
    "            percent_change = float('inf') if experimental_pct > 0 else 0.0\n",
    "        else:\n",
    "            percent_change = ((experimental_pct - control_pct) / control_pct) * 100\n",
    "\n",
    "        stat, pval = proportions_ztest([e, c], [e_total, c_total])\n",
    "\n",
    "        results.append({\n",
    "            \"City\": city_df[\"City\"].iloc[0],\n",
    "            \"Topic\": topic,\n",
    "            \"Control %\": round(control_pct, 1),\n",
    "            \"Experimental %\": round(experimental_pct, 1),\n",
    "            \"Raw Change\": round(raw_change, 1),\n",
    "            \"Percent Change\": round(percent_change, 1),\n",
    "            \"p-value\": round(pval, 4),\n",
    "            \"Significant\": pval < 0.05\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# apply\n",
    "stat_results = pd.concat([compare_topic_shift_by_city(df[df[\"City\"] == city]) for city in cities])\n",
    "stat_results.to_csv(\"topic_significance_by_city_percent.csv\", index=False)"
   ],
   "id": "11b5db8084cde836",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T16:58:08.922943Z",
     "start_time": "2025-05-02T16:58:08.793041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set font style\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "# Load significance results\n",
    "df = pd.read_csv(\"topic_significance_by_city_percent.csv\")\n",
    "\n",
    "# Define focus topics and cities\n",
    "topics = [20, 28, 122, 124, 228]\n",
    "cities = [\"Chicago\", \"Denver\", \"New York City\"]\n",
    "\n",
    "# Filter relevant rows\n",
    "df = df[df[\"Topic\"].isin(topics) & df[\"City\"].isin(cities)].copy()\n",
    "\n",
    "# Create pivot tables\n",
    "pivot_change = df.pivot(index=\"City\", columns=\"Topic\", values=\"Raw Change\").reindex(index=cities, columns=topics)\n",
    "pivot_pval = df.pivot(index=\"City\", columns=\"Topic\", values=\"p-value\").reindex(index=cities, columns=topics)\n",
    "\n",
    "# Normalize p-values for colormap (smaller p = stronger color)\n",
    "norm_pvals = 1 - pivot_pval.clip(upper=0.05) / 0.05\n",
    "cmap = plt.cm.coolwarm\n",
    "\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = cmap(norm_pvals)\n",
    "im = ax.imshow(colors, aspect='auto')\n",
    "\n",
    "# Add annotations (raw change in %)\n",
    "for i in range(len(pivot_change.index)):\n",
    "    for j in range(len(pivot_change.columns)):\n",
    "        val = pivot_change.iloc[i, j]\n",
    "        ax.text(j, i, f\"{val:.1f}%\", ha='center', va='center',\n",
    "                fontsize=11,\n",
    "                color='black' if norm_pvals.iloc[i, j] < 0.4 else 'white')\n",
    "\n",
    "# Format axis labels\n",
    "ax.set_xticks(np.arange(len(topics)))\n",
    "ax.set_xticklabels([f\"Topic {t}\" for t in topics], fontsize=11)\n",
    "\n",
    "ax.set_yticks(np.arange(len(cities)))\n",
    "ax.set_yticklabels([c for c in cities], fontsize=11)\n",
    "\n",
    "# Add gridlines for better readability\n",
    "ax.set_xticks(np.arange(len(topics)+1)-.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(cities)+1)-.5, minor=True)\n",
    "ax.grid(which=\"minor\", color='white', linestyle='-', linewidth=2)\n",
    "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title(\"Change in Topic Prevalence (Experimental − Control)\", fontsize=14, pad=15)\n",
    "ax.set_xlabel(\"Unsupervised Topic\", fontsize=12)\n",
    "ax.set_ylabel(\"City\", fontsize=12)\n",
    "\n",
    "# Add matching colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, label=\"Significance (1 − p)\", shrink=0.85, pad=0.02)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Finalize\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"topic_change_heatmap_readable.png\")\n",
    "plt.close()"
   ],
   "id": "7ec4ddd233030ac7",
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
